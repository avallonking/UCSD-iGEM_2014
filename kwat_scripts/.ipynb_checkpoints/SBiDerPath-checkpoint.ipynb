{
 "metadata": {
  "name": "",
  "signature": "sha256:4a65a67874f5322e6fdc8f8d0a844fbcce7840fb33c08fb476d31ef27a3d81fa"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''finds the last index of an element in a sequence'''\n",
      "def rindex(sequence, element):\n",
      "    for i, e in enumerate(reversed(sequence)):\n",
      "        if element == e:\n",
      "            return len(sequence) - 1 - i\n",
      "    else:\n",
      "            raise ValueError(\"rindex(sequence, element): element not in the sequence\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 130
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''removes the outer most parentheses \"()\" and returns the token afte the \")\"'''\n",
      "def parentheses(sequence):\n",
      "    print \"parentheses(sequence): sequence: \", sequence\n",
      "    first_opener_idx_assigned = False\n",
      "    started = False\n",
      "    counter = 0\n",
      "    for idx, e in enumerate(sequence):\n",
      "        if e == '(':\n",
      "            if started == False:\n",
      "                started = True\n",
      "            counter = counter + 1\n",
      "        elif e == ')':\n",
      "            if started == False:\n",
      "                raise ValueError(\"parentheses(sequence): ')' without '('\")\n",
      "            counter = counter - 1\n",
      "        if started == True:\n",
      "            if first_opener_idx_assigned == False:\n",
      "                first_opener_idx = idx\n",
      "                first_opener_idx_assigned = True\n",
      "            if counter == 0:\n",
      "                sequence.pop(idx)\n",
      "                if idx < len(sequence):\n",
      "                    element_after_last_closer = sequence[idx]\n",
      "                else:\n",
      "                    element_after_last_closer = None\n",
      "                sequence.pop(first_opener_idx)\n",
      "                return element_after_last_closer\n",
      "    print \"parentheses(sequence): no parentehses in the sequence\"\n",
      "    return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''splits a sequence by a given element and stores the elements\n",
      "before and after the element into a dictionary'''\n",
      "def split_by(sequence, element):\n",
      "    element_index = sequence.index(element)\n",
      "    sequence_before_element = sequence[:element_index:1]\n",
      "    sequence_after_element = sequence[element_index + 1::1]\n",
      "    return {0: sequence_before_element, 1: sequence_after_element}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 132
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''class: UISA (User Input Syntax Analyzer)'''\n",
      "class UISA:\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''constructor'''\n",
      "    def __init__(self, tokens):\n",
      "        print \"UISA.__init__(self, tokens): tokens: \", tokens\n",
      "        \"beigns syntax-analyzer-semantic-evaluator recursion\"\n",
      "        print \"RETURN:\\n\", self.g0(tokens)\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''g0:= g1 > g1'''\n",
      "    def g0(self, tokens):\n",
      "        print \"UISA.g(self, tokens): tokens: \", tokens\n",
      "        if '>' not in tokens:\n",
      "            raise ValueError(\"g(self, tokens): no output\")\n",
      "        else:\n",
      "            input_output_dictionary = split_by(tokens, '>')\n",
      "        return self.gOutput(self.g1(input_output_dictionary[0]), self.g1(input_output_dictionary[1]))\n",
      "        \n",
      "    \n",
      "    \n",
      "    '''g1:= g2 or g1 | g2 and g1 | g2'''\n",
      "    def g1(self, tokens):        \n",
      "        print \"UISA.g1(self, tokens): tokens: \", tokens\n",
      "        if len(tokens) > 1 and tokens[1] == 'or':\n",
      "            \"g2 or g1\"\n",
      "            print \"UISA.g0(self, tokens): detected 'or'\"\n",
      "            \"splits tokens by the first occuring 'or' and stores the tokens before and after the 'or' in a dictionary\"\n",
      "            or_dictionary = split_by(tokens, 'or')\n",
      "            return self.gOr(self.g2(or_dictionary.get(0)), self.g1(or_dictionary.get(1)))\n",
      "        elif len(tokens) > 1 and tokens[1] == 'and':\n",
      "            \"g2 and g1\"\n",
      "            print \"UISA.g1(self, tokens): detected 'and'\"\n",
      "            \"splits tokens by the first occuring 'and' and stores the tokens before and after the 'and' in a dictionary\"\n",
      "            and_dictionary = split_by(tokens, 'and')\n",
      "            return self.gAnd(self.g2(and_dictionary.get(0)), self.g1(and_dictionary.get(1)))            \n",
      "        else:\n",
      "            \"g2\"\n",
      "            \"delegates to g2\"\n",
      "            return self.g2(tokens)\n",
      "        \n",
      "        \n",
      "        \n",
      "    '''g2:= (g1) or g1 | (g1) and g1 | (g1) | interactor''' \n",
      "    def g2(self, tokens):\n",
      "        print \"UISA.g2(self, tokens): tokens: \", tokens\n",
      "        if tokens[0] == \"(\":\n",
      "            \"(g1) or g1 | (g1) and g1| (g1)\"\n",
      "            print \"UISA.g2(self, tokens): detected '('\"\n",
      "            \"token after the last occuring ')'\"\n",
      "            token_after_last_closer = parentheses(tokens)\n",
      "            if token_after_last_closer == 'or':    \n",
      "                \"splits tokens by the first occuring 'or' and stores the tokens before and after the 'or' in a dictionary\"\n",
      "                or_dictionary = split_by(tokens, 'or')\n",
      "                return self.gOr(self.g1(or_dictionary.get(0)), self.g1(or_dictionary.get(1)))\n",
      "            elif token_after_last_closer == 'and':\n",
      "                \"splits tokens by the first occuring 'and' and stores the tokens before and after the 'and' in a dictionary\"\n",
      "                and_dictionary = split_by(tokens, 'and')\n",
      "                return self.gAnd(self.g1(and_dictionary.get(0)), self.g1(and_dictionary.get(1)))\n",
      "            else:\n",
      "                \"delegates to interactor\"\n",
      "                return self.g1(tokens)\n",
      "        else:\n",
      "            \"interactor\"\n",
      "            \"delegates to interactor\"\n",
      "            return self.interactor(tokens)\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''interactor:= lacI | cI | ... | etc.'''\n",
      "    def interactor(self, tokens):\n",
      "        print \"UISA.interactor(self, tokens): tokens: \", tokens\n",
      "        \"returns an interactor\"\n",
      "        return tokens\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''>'''\n",
      "    def gOutput(self, tokens1, tokens2):\n",
      "        toBeReturned = []\n",
      "        for token1 in tokens1:\n",
      "            for token2 in tokens2:\n",
      "                toBeReturned.append(('[' + token1 + '] ---> [' + token2 + ']'))\n",
      "        return toBeReturned\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''Or'''\n",
      "    def gOr(self, tokens1, tokens2):\n",
      "        return tokens1 + tokens2\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''And'''\n",
      "    def gAnd(self, tokens1, tokens2):\n",
      "        toBeReturned = []\n",
      "        for token1 in tokens1:\n",
      "            for token2 in tokens2:\n",
      "                toBeReturned.append(token1 + (\" and \" + token2))\n",
      "        return toBeReturned"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 133
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''main'''\n",
      "def main1(user_input):\n",
      "    user_input_token= user_input.split()\n",
      "    uisa = UISA(user_input_token)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 134
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def uniquelyMerge(mainList, mergingList):\n",
      "    uniquelyMergedList = mainList\n",
      "    for e in mergingList:\n",
      "        if e not in mainList:\n",
      "            uniquelyMergedList.append(e)\n",
      "    return uniquelyMergedList"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 135
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def getPath(inputTransitionDictionary, outputTransitionDictionary, startSpecies, endSpecies):    \n",
      "    pathQueue=[ ([],[startSpecies]) ]    \n",
      "    finalSpeciesPathList = []\n",
      "    finalTransitionPathList = []\n",
      "    while pathQueue != []:\n",
      "        pathQueue, finalSpeciesPathList, finalTransitionPathList = traverse(inputTransitionDictionary,\n",
      "                                                            outputTransitionDictionary,\n",
      "                                                            startSpecies,\n",
      "                                                            endSpecies,\n",
      "                                                            pathQueue,\n",
      "                                                            finalSpeciesPathList,\n",
      "                                                            finalTransitionPathList)\n",
      "    return finalSpeciesPathList, finalTransitionPathList"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 136
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def traverse(inputTransitionDictionary,\n",
      "            outputTransitionDictionary,\n",
      "            startSpecies,\n",
      "            endSpecies,\n",
      "            pathQueue,\n",
      "            finalSpeciesPathList, \n",
      "            finalTransitionPathList):\n",
      "    \n",
      "    (visitedTransition, visitedSpecies) = pathQueue.pop(0)\n",
      "    inputSpeciesList = [startSpecies]\n",
      "    \n",
      "    # Output of visited transitions becomes new input\n",
      "    for aVisitedTransition in visitedTransition:\n",
      "        outputSpecies = outputTransitionDictionary[aVisitedTransition]\n",
      "        for anOutputSpecies in outputSpecies:\n",
      "            inputSpeciesList.append(anOutputSpecies)\n",
      "    \n",
      "    # Check transitions that have not been checked\n",
      "    for aTransition in set(inputTransitionDictionary.keys()) - set(visitedTransition):\n",
      "        \n",
      "        # Input of an unchecked transition\n",
      "        potentialInputSpecies = inputTransitionDictionary[aTransition]\n",
      "        \n",
      "        # If input of an unchecked transition matches input species\n",
      "        if potentialInputSpecies.issubset( set(inputSpeciesList) ) == True:\n",
      "                         \n",
      "            # Include this transition in current transition path list\n",
      "            aTransitionPathList = []\n",
      "            aTransitionPathList = aTransitionPathList + visitedTransition + [aTransition]\n",
      "                  \n",
      "            # Include unique species of this transition in current species path list\n",
      "            aSpeciesPathList = []\n",
      "            aSpeciesPathList = uniquelyMerge(aSpeciesPathList + visitedSpecies, outputTransitionDictionary[aTransition])\n",
      "\n",
      "            # If end speices is found, return species path and transition path\n",
      "            if set(endSpecies).issubset( set(aSpeciesPathList) ):\n",
      "                finalSpeciesPathList.append(aSpeciesPathList)\n",
      "                finalTransitionPathList.append(aTransitionPathList)\n",
      "                \n",
      "            # If end species is not found, add species path and transition path to path queue\n",
      "            else:\n",
      "                pathQueue.append( (aTransitionPathList, aSpeciesPathList) )\n",
      "    \n",
      "    return pathQueue, finalSpeciesPathList, finalTransitionPathList"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 137
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "inputTrans = {\n",
      "#input transition ID : set([intermediates required to trigger the transition])\n",
      "'t0':set( ['A'] ),\n",
      "'t1':set( ['B'] ),\n",
      "'t2':set( ['C'] ),\n",
      "'t3':set( ['D'] ),\n",
      "'t4':set( ['E'] )\n",
      "}\n",
      "\n",
      "outputTrans = {\n",
      "#output transition ID : [intermediates produced by the transition]\n",
      "'t0':['B'],\n",
      "'t1':['C'],\n",
      "'t2':['D'],\n",
      "'t3':['E'],\n",
      "'t4':['X']\n",
      "}\n",
      "\n",
      "user_input = \"A > X\"\n",
      "\n",
      "print getPath(inputTrans, outputTrans, 'A', ['X'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "([['A', 'B', 'C', 'D', 'E', 'X']], [['t0', 't1', 't2', 't3', 't4']])\n"
       ]
      }
     ],
     "prompt_number": 138
    }
   ],
   "metadata": {}
  }
 ]
}