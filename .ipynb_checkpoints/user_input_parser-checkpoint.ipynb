{
 "metadata": {
  "name": "",
  "signature": "sha256:0015c7bb5ba7a609577c47c4e82e70e4a3b1fc10f07b295a8a0296f87f5e68db"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''User Input Syntax Analyzer and Semantic Evaluator'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 507,
       "text": [
        "'User Input Syntax Analyzer and Semantic Evaluator'"
       ]
      }
     ],
     "prompt_number": 507
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''user input test cases'''\n",
      "user_input_1 = \"A > X\"\n",
      "user_input_2 = \"A or B > X\"\n",
      "user_input_3 = \"A and B > X\"\n",
      "user_input_4 = \"( A and B ) or C > X\"\n",
      "user_input_5 = \"A and ( B or C ) > X\"\n",
      "user_input_6 = \"A and B and C > X\"\n",
      "user_input_7 = \"( A ) > X\"\n",
      "user_input_8 = \"A or B or C > X\"\n",
      "user_input_9 = \"A or B or C > X\"\n",
      "user_input_10 = \"( A ) or ( B ) > X\"\n",
      "user_input_11 = \"( A ) and ( B ) > X\"\n",
      "user_input_12 = \"( A or B and ( C or D or ( E and F or ( G or H ) and I ) or J  ) or K ) or L > X\"\n",
      "user_input_13 = \"( A )\"\n",
      "\n",
      "for i in range(1, 14):\n",
      "    exec(\"toks%d = user_input_%d.split()\" % (i, i))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 508
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''finds the index of the last occurance of a value in a sequence'''\n",
      "def rindex(sequence, item):\n",
      "    for i, e in enumerate(reversed(sequence)):\n",
      "        if item == e:\n",
      "            return len(sequence) - 1 - i\n",
      "    else:\n",
      "            raise ValueError(\"rindex(sequence, item): item not in sequence\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 509
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''removes the outer most parentheses \"()\" and returns the token afte the \")\"'''\n",
      "def parentheses(sequence):\n",
      "    print \"initial sequence --- \", sequence\n",
      "    first_opener_idx = None\n",
      "    first_opener_idx_assigned = False\n",
      "    token_after_last_closer = None\n",
      "    counter = 0\n",
      "    started = False\n",
      "    for idx, token in enumerate(sequence):\n",
      "\n",
      "        if token == '(':\n",
      "            if started == False:\n",
      "                started = True\n",
      "            counter = counter + 1\n",
      "        elif token == ')':\n",
      "            if started == False:\n",
      "                raise ValueError(\"parentheses(sequence): invalid syntax\")\n",
      "            counter = counter - 1\n",
      "        \n",
      "        if started == True:\n",
      "            \n",
      "            if first_opener_idx_assigned == False:\n",
      "                first_opener_idx = idx\n",
      "                first_opener_idx_assigned = True\n",
      "                '''print \"first opener idx is assigned to be: \", first_opener_idx'''\n",
      "            \n",
      "            if counter == 0:\n",
      "                '''print \"popping \", sequence[idx], \"at index \", idx'''\n",
      "                sequence.pop(idx)\n",
      "                \n",
      "                if idx < len(sequence):\n",
      "                    token_after_last_closer = sequence[idx]\n",
      "                else:\n",
      "                    token_after_last_closer = \"none - end of sequence\"\n",
      "                \n",
      "                '''print \"popping \", sequence[first_opener_idx], \"at index \", first_opener_idx'''\n",
      "                sequence.pop(first_opener_idx)\n",
      "                \n",
      "                '''print \"final sequence --- \", sequence'''\n",
      "                \n",
      "                return token_after_last_closer\n",
      "\n",
      "    print \"no parentehses in sequence\"\n",
      "    return None"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 510
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''class: UISA (User Input Syntax Analyzer)'''\n",
      "class UISA:\n",
      "    \n",
      "    '''constructor'''\n",
      "    def __init__(self, tokens):\n",
      "        self.tokens = tokens\n",
      "        print \"A UISA is initialied with tokens:   \", tokens\n",
      "        \n",
      "        'beigns the recursion'\n",
      "        self.g0(self.tokens)\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''g0:= g2 or g0 | g1'''    \n",
      "    def g0(self, tks):\n",
      "        print \"g0 --- tokens:   \", tks\n",
      "        if len(tks) > 1 and tks[1] == \"or\":\n",
      "            'g2 or g0'\n",
      "            print \"!g0 --- detected 'or'\"\n",
      "            \n",
      "            'splits tokens by the first occuring \"or\" and \\\n",
      "             stores the tokens before and after the \"or\" in a dictionary.'\n",
      "            or_dictionary = self.split_by(tks, \"or\")\n",
      "            \n",
      "            'returns a GOr object with the tokens in the or_dictionary'\n",
      "            gor = GOr(self.g2(or_dictionary.get(\"before\")), self.g0(or_dictionary.get(\"after\")))\n",
      "            return gor\n",
      "        else:\n",
      "            'g1'\n",
      "            \n",
      "            'delegates to g1.'\n",
      "            return self.g1(tks)\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''g1:= g2 and g0 | g2'''\n",
      "    def g1(self, tks):\n",
      "        print \"g1 --- tokens:   \", tks\n",
      "        if len(tks) > 1 and tks[1] == \"and\":\n",
      "            'g2 and g0'\n",
      "            print \"!g1 --- detected 'and'\"\n",
      "            \n",
      "            'splits tokens by the first occuring \"and\" and \\\n",
      "             stores the tokens before and after the \"and\" in a dictionary.'\n",
      "            and_dictionary = self.split_by(tks, \"and\")\n",
      "            \n",
      "            'returns a GOr object with the tokens in the or_dictionary'\n",
      "            gand = GAnd(and_dictionary.get(\"before\"), and_dictionary.get(\"after\"))\n",
      "            return gand\n",
      "        else:\n",
      "            'g2'\n",
      "            \n",
      "            'delegates to g2.'\n",
      "            return self.g2(tks)\n",
      "    \n",
      "    \n",
      "    \n",
      "    '''2:=  (g0) | (g0) or ... | (g0) and ... | interactor''' \n",
      "    def g2(self, tks):\n",
      "        print \"g2 --- tokens:   \", tks\n",
      "        if tks[0] == \"(\":\n",
      "            '(g0)'\n",
      "            print \"!!!!!!!!!!!!!!g2 --- detected '('\"\n",
      "            \n",
      "            'token after the last occuring \")\"'\n",
      "            token_after_last_closer = parentheses(tks)\n",
      "            print \"!!!!!!!!!!!!!!!!!!!!!!!!!!!!! token right after the ')' is: \", token_after_last_closer\n",
      "            \n",
      "            if token_after_last_closer == \"or\":    \n",
      "\n",
      "                'splits tokens by \"or\" and \\\n",
      "                stores the tokens before and after the \"or\" in a dictionary.'\n",
      "                or_dictionary = self.split_by(tks, \"or\")\n",
      "\n",
      "                'returns a GOr object with the tokens in the or_dictionary'\n",
      "                gor = GOr(self.g2(or_dictionary.get(\"before\")), self.g0(or_dictionary.get(\"after\")))\n",
      "                return gor\n",
      "\n",
      "            elif token_after_last_closer == \"and\":\n",
      "\n",
      "                'splits tokens by the first occuring \"and\" and \\\n",
      "                stores the tokens before and after the \"and\" in a dictionary.'\n",
      "                and_dictionary = self.split_by(tks, \"and\")\n",
      "\n",
      "                'returns a GOr object with the tokens in the or_dictionary'\n",
      "                gand = GAnd(and_dictionary.get(\"before\"), and_dictionary.get(\"after\"))\n",
      "                return gand\n",
      "            else:\n",
      "                print \"')' IS THE LAST TOKEN, AND NOTHING AFTER\"\n",
      "                return self.g0(tks)\n",
      "        else:\n",
      "            'interactor'\n",
      "            \n",
      "            'delegates to interactor'\n",
      "            return self.interactor(tks)\n",
      "\n",
      "    \n",
      "    \n",
      "    '''interactor : = lacI | cI | ... | ... | etc.'''\n",
      "    def interactor(self, tks):\n",
      "        print \"interactor --- tokens:   \", tks\n",
      "\n",
      "        'return the name of an interactor'\n",
      "        return \"INTERACTOR\"\n",
      "    \n",
      "    \n",
      "    ''' splits tokens by a given token and stores the tokens /\n",
      "        before and after the token in a dictionary '''\n",
      "    def split_by(self, tokens, tk):\n",
      "        tk_index = tokens.index(tk)\n",
      "        tokens_before_tk = tokens[:tk_index]\n",
      "        tokens_after_tk = tokens[tk_index + 1:]\n",
      "        print \"before split_by:   \", tokens_before_tk\n",
      "        print \"after split_by:   \", tokens_after_tk\n",
      "        return {'before': tokens_before_tk, 'after': tokens_after_tk}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 511
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Class: GOr (Grammar Or)'''\n",
      "class GOr:\n",
      "    def __init__(self, tokens_before_logic, tokens_after_logic):\n",
      "        self.tokens_before_logic = tokens_before_logic\n",
      "        self.tokens_after_logic = tokens_after_logic\n",
      "        print \"Gor - tokens_before_logic:   \", tokens_before_logic\n",
      "        print \"Gor - tokens_after_logic:   \", tokens_after_logic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 512
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''Class: Gand (Grammar And)'''\n",
      "class GAnd:\n",
      "    def __init__(self, tokens_before_logic, tokens_after_logic):\n",
      "        self.tokens_before_logic = tokens_before_logic\n",
      "        self.tokens_after_logic = tokens_after_logic\n",
      "        print \"Gand - tokens_before_logic:   \", tokens_before_logic\n",
      "        print \"Gand - tokens_after_logic:   \", tokens_after_logic"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 513
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''main'''\n",
      "def main(user_input):\n",
      "    if '>' in user_input:\n",
      "        suffix = user_input[user_input.index('>')::1]\n",
      "    else:\n",
      "        raise ValueError(\"main(user_input): missing output(s) '> ...'\")\n",
      "        return\n",
      "    tokens = user_input[:user_input.index('>'):1]\n",
      "    uisa = UISA(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 524
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "main(toks13)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "main(user_input): missing output(s) '> ...'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-525-77fbd5adca6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoks13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-524-a131e4c0112b>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(user_input)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"main(user_input): missing output(s) '> ...'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: main(user_input): missing output(s) '> ...'"
       ]
      }
     ],
     "prompt_number": 525
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}